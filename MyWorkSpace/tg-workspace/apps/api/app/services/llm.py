"""
LLM Service for Message Generation and AI Coaching
Uses Google Gemini via OpenAI-compatible API
"""
import os
import json
import re
from typing import Dict, Any, List, Optional
from openai import OpenAI
import logging

logger = logging.getLogger(__name__)

GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"
FLASH_MODEL = "gemini-2.5-flash"
PRO_MODEL = "gemini-2.5-pro"


def get_client() -> OpenAI:
    """Get OpenAI client configured for Gemini"""
    api_key = os.getenv("GEMINI_API_KEY", "")
    if not api_key:
        raise ValueError("GEMINI_API_KEY not set")
    
    return OpenAI(api_key=api_key, base_url=GEMINI_BASE_URL)


def should_use_pro(context: Dict[str, Any]) -> bool:
    """
    Decide whether to use Pro model based on context complexity
    """
    # Use Pro for:
    # - Objection handling
    # - Complex conversations (multiple exchanges)
    # - High-value leads
    # - Coach recommendations
    
    if context.get('has_objection', False):
        return True
    if context.get('conversation_length', 0) > 3:
        return True
    if context.get('money_score', 0) > 0.8:
        return True
    if context.get('task_type') in ['coach', 'objection', 'strategy']:
        return True
    
    return False


def classify_message_llm(text: str) -> Dict[str, Any]:
    """
    Use Gemini to classify message as tech task for automation specialist.
    Focuses on: bots, automation, AI, integrations, parsing, CRM.
    
    Returns:
        {
            "is_tech_task": bool,
            "confidence": float (0-1),
            "task_type": str (bot/automation/integration/parsing/ai/other),
            "tech_keywords": list[str],
            "budget_hint": str or None,
            "urgency": str (low/medium/high),
            "reason": str
        }
    """
    if not text or len(text.strip()) < 20:
        return {
            "is_tech_task": False,
            "confidence": 0.0,
            "task_type": "unknown",
            "tech_keywords": [],
            "budget_hint": None,
            "urgency": "low",
            "reason": "Text too short"
        }
    
    try:
        client = get_client()
    except ValueError:
        # No API key - return neutral result
        return {
            "is_tech_task": False,
            "confidence": 0.0,
            "task_type": "unknown",
            "tech_keywords": [],
            "budget_hint": None,
            "urgency": "low",
            "reason": "No API key configured"
        }
    
    prompt = f"""ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¸Ð· Telegram-Ñ‡Ð°Ñ‚Ð° Ð¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ ÑÑ‚Ð¾ Ð—ÐÐšÐÐ—ÐžÐœ/Ð—ÐÐ”ÐÐ§Ð•Ð™ Ð´Ð»Ñ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ð° Ð¿Ð¾ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸.

Ð¡ÐžÐžÐ‘Ð©Ð•ÐÐ˜Ð•:
\"\"\"{text[:1500]}\"\"\"

ÐœÐµÐ½Ñ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÑƒÑŽÑ‚ Ð¢ÐžÐ›Ð¬ÐšÐž Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð½Ð°:
- Telegram/WhatsApp/VK Ð±Ð¾Ñ‚Ñ‹ (aiogram, salebot, manychat)
- ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² (n8n, make, zapier)
- Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸ CRM (amocrm, bitrix, notion)
- ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð°Ð½Ð½Ñ‹Ñ…, ÑÐºÑ€Ð°Ð¿Ð¸Ð½Ð³
- AI/GPT Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸, Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚Ð¸
- Ð¡ÐºÑ€Ð¸Ð¿Ñ‚Ñ‹ Ð½Ð° Python/JavaScript
- ÐÐ²Ñ‚Ð¾Ð¿Ð¾ÑÑ‚Ð¸Ð½Ð³, ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚-Ð·Ð°Ð²Ð¾Ð´Ñ‹
- GetCourse, Prodamus, Ð²Ð¾Ñ€Ð¾Ð½ÐºÐ¸

ÐÐ• Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÑƒÑŽÑ‚:
- "#Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ" - Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ ÑƒÑÐ»ÑƒÐ³ Ð¾Ñ‚ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð²
- Ð’Ð°ÐºÐ°Ð½ÑÐ¸Ð¸ Ð½Ð° SMM, Ð´Ð¸Ð·Ð°Ð¹Ð½, Ñ‚Ð°Ñ€Ð³ÐµÑ‚, ÐºÐ¾Ð¿Ð¸Ñ€Ð°Ð¹Ñ‚
- ÐŸÐ¾Ð¸ÑÐº Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð¼
- Ð¡Ð¿Ð°Ð¼, Ñ€ÐµÐºÐ»Ð°Ð¼Ð°, ÐºÑ€Ð¸Ð¿Ñ‚Ð°
- ÐŸÑ€Ð¾ÑÑ‚Ð¾ Ð±Ð¾Ð»Ñ‚Ð¾Ð²Ð½Ñ/Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð¡Ð¢Ð ÐžÐ“Ðž Ð² JSON:
{{
    "is_tech_task": true/false,
    "confidence": 0.0-1.0,
    "task_type": "bot|automation|integration|parsing|ai|site|other",
    "tech_keywords": ["keyword1", "keyword2"],
    "budget_hint": "50000 Ñ€ÑƒÐ±" Ð¸Ð»Ð¸ null,
    "urgency": "low|medium|high",
    "reason": "ÐšÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ"
}}"""
    
    try:
        response = client.chat.completions.create(
            model=FLASH_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,  # Low temp for consistent classification
            max_tokens=400,
        )
        
        content = response.choices[0].message.content.strip()
        json_match = re.search(r'\{[\s\S]*\}', content)
        
        if json_match:
            result = json.loads(json_match.group())
            # Validate required fields
            result.setdefault("is_tech_task", False)
            result.setdefault("confidence", 0.5)
            result.setdefault("task_type", "other")
            result.setdefault("tech_keywords", [])
            result.setdefault("budget_hint", None)
            result.setdefault("urgency", "low")
            result.setdefault("reason", "")
            return result
        else:
            return {
                "is_tech_task": False,
                "confidence": 0.3,
                "task_type": "unknown",
                "tech_keywords": [],
                "budget_hint": None,
                "urgency": "low",
                "reason": "Failed to parse LLM response"
            }
            
    except Exception as e:
        logger.error(f"LLM classification error: {e}")
        return {
            "is_tech_task": False,
            "confidence": 0.0,
            "task_type": "error",
            "tech_keywords": [],
            "budget_hint": None,
            "urgency": "low",
            "reason": str(e)
        }


def generate_outreach_message(
    lead_text: str,
    lead_author: str,
    category: str,
    template: Optional[str] = None,
    offers: Optional[List[str]] = None,
    previous_messages: Optional[List[str]] = None,
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate personalized outreach message for a lead
    Returns message text and metadata
    """
    client = get_client()
    context = context or {}
    model = PRO_MODEL if should_use_pro(context) else FLASH_MODEL
    
    # Build context for prompt
    offers_text = "\n".join(offers) if offers else "ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°: Ð±Ð¾Ñ‚Ñ‹, Ð¿Ð°Ñ€ÑÐµÑ€Ñ‹, Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸, CRM"
    prev_text = "\n".join(previous_messages) if previous_messages else "ÐŸÐµÑ€Ð²Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ"
    
    prompt = f"""Ð¢Ñ‹ - Ð¾Ð¿Ñ‹Ñ‚Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð´Ð°Ð²ÐµÑ† IT-ÑƒÑÐ»ÑƒÐ³. ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Ð¸Ð· Telegram.

ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢ Ð›Ð˜Ð”Ð:
ÐÐ²Ñ‚Ð¾Ñ€: {lead_author}
Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ: {lead_text}
ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ: {category}

ÐœÐžÐ˜ Ð£Ð¡Ð›Ð£Ð“Ð˜:
{offers_text}

ÐŸÐ Ð•Ð”Ð«Ð”Ð£Ð©ÐÐ¯ ÐŸÐ•Ð Ð•ÐŸÐ˜Ð¡ÐšÐ:
{prev_text}

{'Ð¨ÐÐ‘Ð›ÐžÐ Ð”Ð›Ð¯ ÐÐ”ÐÐŸÐ¢ÐÐ¦Ð˜Ð˜: ' + template if template else ''}

ÐŸÐ ÐÐ’Ð˜Ð›Ð:
1. Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¼ (2-4 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ)
2. ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð¿Ð¾Ð´ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ
3. ÐÐµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹ Ñ‚Ð¸Ð¿Ð° "Ð”Ð¾Ð±Ñ€Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ! Ð£Ð²Ð¸Ð´ÐµÐ» Ð²Ð°ÑˆÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ..."
4. ÐŸÐ¾ÐºÐ°Ð¶Ð¸ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¸Ð·Ñƒ, Ð½Ð¾ Ð½Ðµ Ñ…Ð²Ð°ÑÑ‚Ð°Ð¹ÑÑ
5. Ð—Ð°Ð´Ð°Ð¹ ÑƒÑ‚Ð¾Ñ‡Ð½ÑÑŽÑ‰Ð¸Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸Ð»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³
6. Ð¢Ð¾Ð½: Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¹, Ð½Ð¾ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹
7. ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÐ¼Ð¾Ð´Ð·Ð¸ Ð² Ð¸Ð·Ð±Ñ‹Ñ‚ÐºÐµ (Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ 1-2)
8. Ð”Ð¾Ð±Ð°Ð²ÑŒ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ (Ñ†Ð¸Ñ„Ñ€Ñ‹, ÐºÐµÐ¹ÑÑ‹, ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð¸ÐºÑƒ)

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð² JSON:
{{
    "message": "Ð¢ÐµÐºÑÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ",
    "hook": "Ð§Ñ‚Ð¾ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð·Ð°Ñ†ÐµÐ¿Ð¸Ð»Ð¸ Ð² Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ",
    "next_step": "ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³",
    "personalization_points": ["Ð¢Ð¾Ñ‡ÐºÐ° 1", "Ð¢Ð¾Ñ‡ÐºÐ° 2"]
}}"""

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=800,
        )
        
        content = response.choices[0].message.content.strip()
        
        # Parse JSON
        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            result = json.loads(json_match.group())
            result['model_used'] = model
            return result
        else:
            return {
                "message": content,
                "hook": "",
                "next_step": "",
                "personalization_points": [],
                "model_used": model
            }
            
    except Exception as e:
        logger.error(f"Message generation error: {e}")
        return {
            "message": "",
            "error": str(e),
            "model_used": model
        }


def handle_objection(
    objection_text: str,
    lead_context: str,
    previous_attempts: List[str] = None
) -> Dict[str, Any]:
    """
    Generate response to client objection
    Always uses Pro model for better quality
    """
    client = get_client()
    
    prev_text = "\n".join(previous_attempts) if previous_attempts else "ÐÐµÑ‚ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº"
    
    prompt = f"""ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð²Ñ‹Ð´Ð²Ð¸Ð½ÑƒÐ» Ð²Ð¾Ð·Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ. ÐŸÐ¾Ð¼Ð¾Ð³Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ.

ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢:
{lead_context}

Ð’ÐžÐ—Ð ÐÐ–Ð•ÐÐ˜Ð• ÐšÐ›Ð˜Ð•ÐÐ¢Ð:
{objection_text}

ÐŸÐ Ð•Ð”Ð«Ð”Ð£Ð©Ð˜Ð• ÐžÐ¢Ð’Ð•Ð¢Ð«:
{prev_text}

Ð”Ð°Ð¹ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸ÑŽ Ð¿Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ Ð²Ð¾Ð·Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°.
ÐžÑ‚Ð²ÐµÑ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½:
1. ÐŸÑ€Ð¸Ð·Ð½Ð°Ñ‚ÑŒ Ñ‚Ð¾Ñ‡ÐºÑƒ Ð·Ñ€ÐµÐ½Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
2. ÐœÑÐ³ÐºÐ¾ Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð²Ð¾Ð·Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ
3. Ð”Ð°Ñ‚ÑŒ Ñ†ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð¸Ð»Ð¸ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚
4. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³

JSON Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚:
{{
    "objection_type": "Ñ†ÐµÐ½Ð°/ÑÑ€Ð¾ÐºÐ¸/Ð´Ð¾Ð²ÐµÑ€Ð¸Ðµ/Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚/Ð´Ñ€ÑƒÐ³Ð¾Ðµ",
    "response": "Ð¢ÐµÐºÑÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°",
    "strategy": "ÐšÐ°ÐºÑƒÑŽ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»Ð¸",
    "alternative_responses": ["Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2", "Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 3"],
    "tips": ["Ð¡Ð¾Ð²ÐµÑ‚ 1", "Ð¡Ð¾Ð²ÐµÑ‚ 2"]
}}"""

    try:
        response = client.chat.completions.create(
            model=PRO_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=1000,
        )
        
        content = response.choices[0].message.content.strip()
        json_match = re.search(r'\{[\s\S]*\}', content)
        
        if json_match:
            return json.loads(json_match.group())
        else:
            return {"response": content, "objection_type": "unknown"}
            
    except Exception as e:
        logger.error(f"Objection handling error: {e}")
        return {"error": str(e)}


def get_sales_coach_advice(
    lead_info: Dict[str, Any],
    current_status: str,
    history: List[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Get AI Sales Coach recommendation for next action
    """
    client = get_client()
    
    history_text = json.dumps(history, ensure_ascii=False, indent=2) if history else "ÐÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸"
    
    prompt = f"""Ð¢Ñ‹ AI Sales Coach. ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð»Ð¸Ð´ Ð¸ Ð´Ð°Ð¹ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸.

Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð¯ Ðž Ð›Ð˜Ð”Ð•:
{json.dumps(lead_info, ensure_ascii=False, indent=2)}

Ð¢Ð•ÐšÐ£Ð©Ð˜Ð™ Ð¡Ð¢ÐÐ¢Ð£Ð¡: {current_status}

Ð˜Ð¡Ð¢ÐžÐ Ð˜Ð¯ Ð’Ð—ÐÐ˜ÐœÐžÐ”Ð•Ð™Ð¡Ð¢Ð’Ð˜Ð™:
{history_text}

Ð”Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ðµ, actionable Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸:
1. Ð§Ñ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ð´Ð°Ð»ÑŒÑˆÐµ?
2. ÐšÐ°ÐºÐ¾Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ?
3. ÐšÐ°ÐºÐ¸Ðµ Ñ€Ð¸ÑÐºÐ¸ ÑƒÑ‡ÐµÑÑ‚ÑŒ?
4. ÐžÑ†ÐµÐ½ÐºÐ° Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ ÑƒÑÐ¿ÐµÑ…Ð°

JSON:
{{
    "next_action": "ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ",
    "approach": "Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´",
    "timing": "ÐšÐ¾Ð³Ð´Ð° Ð»ÑƒÑ‡ÑˆÐµ ÑÐ²ÑÐ·Ð°Ñ‚ÑŒÑÑ",
    "risks": ["Ð Ð¸ÑÐº 1"],
    "success_probability": 0.7,
    "one_liner_tip": "ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ ÑÐ¾Ð²ÐµÑ‚ Ð² Ð¾Ð´Ð½Ð¾ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ"
}}"""

    try:
        response = client.chat.completions.create(
            model=FLASH_MODEL,  # Coach can use Flash for speed
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=600,
        )
        
        content = response.choices[0].message.content.strip()
        json_match = re.search(r'\{[\s\S]*\}', content)
        
        if json_match:
            return json.loads(json_match.group())
        else:
            return {"one_liner_tip": content}
            
    except Exception as e:
        logger.error(f"Coach advice error: {e}")
        return {"error": str(e)}


def generate_daily_summary(stats: Dict[str, Any]) -> str:
    """
    Generate end-of-day summary with AI insights
    """
    client = get_client()
    
    prompt = f"""Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ðµ (2-3 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ) ÐµÐ¶ÐµÐ´Ð½ÐµÐ²Ð½Ð¾Ðµ Ñ€ÐµÐ·ÑŽÐ¼Ðµ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ CRM.

Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ Ð”ÐÐ¯:
- ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹: {stats.get('messages_sent', 0)}
- ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²: {stats.get('replies', 0)}
- Ð”Ð²Ð¸Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ð²Ð¾Ñ€Ð¾Ð½ÐºÐµ: {stats.get('funnel_moves', 0)}
- ÐÐ¾Ð²Ñ‹Ñ… Ð»Ð¸Ð´Ð¾Ð²: {stats.get('new_leads', 0)}
- Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¾ ÑÐ´ÐµÐ»Ð¾Ðº: {stats.get('won', 0)}

Ð ÐµÐ·ÑŽÐ¼Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾:
1. ÐŸÐ¾Ñ…Ð²Ð°Ð»Ð¸Ñ‚ÑŒ Ð·Ð° ÑƒÑÐ¿ÐµÑ…Ð¸ Ð¸Ð»Ð¸ Ð¼ÑÐ³ÐºÐ¾ Ð¼Ð¾Ñ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ
2. Ð”Ð°Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ ÑÐ¾Ð²ÐµÑ‚ Ð½Ð° Ð·Ð°Ð²Ñ‚Ñ€Ð°
3. Ð‘Ñ‹Ñ‚ÑŒ Ð¿Ð¾Ð·Ð¸Ñ‚Ð¸Ð²Ð½Ñ‹Ð¼, Ð½Ð¾ Ð½Ðµ Ð¿Ñ€Ð¸Ñ‚Ð¾Ñ€Ð½Ñ‹Ð¼

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ Ñ€ÐµÐ·ÑŽÐ¼Ðµ, Ð±ÐµÐ· JSON."""

    try:
        response = client.chat.completions.create(
            model=FLASH_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            max_tokens=200,
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"Summary generation error: {e}")
        return f"Ð¡ÐµÐ³Ð¾Ð´Ð½Ñ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ {stats.get('messages_sent', 0)} ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹. ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°Ð¹ Ð² Ñ‚Ð¾Ð¼ Ð¶Ðµ Ð´ÑƒÑ…Ðµ! ðŸ’ª"


def uniqualize_message(template: str, variations: int = 3) -> List[str]:
    """
    Generate unique variations of a message template
    For anti-spam protection
    """
    client = get_client()
    
    prompt = f"""Ð¡Ð¾Ð·Ð´Ð°Ð¹ {variations} ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð° ÑÑ‚Ð¾Ð³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ.
Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸ ÑÐ¼Ñ‹ÑÐ», Ð½Ð¾ Ð¸Ð·Ð¼ÐµÐ½Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ, ÑÐ»Ð¾Ð²Ð°, Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº.
ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð²Ñ‹Ð³Ð»ÑÐ´ÐµÑ‚ÑŒ ÐºÐ°Ðº Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð½Ñ‹Ð¹ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ.

ÐžÐ Ð˜Ð“Ð˜ÐÐÐ›:
{template}

ÐžÑ‚Ð²ÐµÑ‚ÑŒ JSON ÑÐ¿Ð¸ÑÐºÐ¾Ð¼:
["Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 1", "Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2", "Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 3"]"""

    try:
        response = client.chat.completions.create(
            model=FLASH_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.9,
            max_tokens=1000,
        )
        
        content = response.choices[0].message.content.strip()
        json_match = re.search(r'\[[\s\S]*\]', content)
        
        if json_match:
            return json.loads(json_match.group())
        else:
            return [template]
            
    except Exception as e:
        logger.error(f"Uniqualization error: {e}")
        return [template]

def paraphrase_message(text: str) -> str:
    """
    Rewrite text to make it unique while preserving meaning
    """
    client = get_client()
    
    prompt = f"""ÐŸÐµÑ€ÐµÐ¿Ð¸ÑˆÐ¸ ÑÑ‚Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸, Ð½Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸ ÑÐ¼Ñ‹ÑÐ» Ð¸ Ñ‚Ð¾Ð½.
Ð­Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ Ð´Ð»Ñ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¸ Ð² Ð´Ñ€ÑƒÐ³Ð¾Ð¹ Ñ‡Ð°Ñ‚, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ‚ÐµÐºÑÑ‚ Ð½Ðµ Ð±Ñ‹Ð» Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ‡Ð½Ñ‹Ð¼ Ð´ÑƒÐ±Ð»ÐµÐ¼.
ÐÐµ Ð¼ÐµÐ½ÑÐ¹ ÐºÐ»ÑŽÑ‡ÐµÐ²ÑƒÑŽ ÑÑƒÑ‚ÑŒ (Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ñ, ÑƒÑÐ»Ð¾Ð²Ð¸Ñ, ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹).
Ð”ÐµÐ»Ð°Ð¹ Ñ‚ÐµÐºÑÑ‚ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¼, ÐºÐ°Ðº Ð±ÑƒÐ´Ñ‚Ð¾ ÐµÐ³Ð¾ Ð½Ð°Ð¿Ð¸ÑÐ°Ð» Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº.

ÐžÐ Ð˜Ð“Ð˜ÐÐÐ›:
{text}

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð¢ÐžÐ›Ð¬ÐšÐž Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ Ð½Ð¾Ð²Ð¾Ð³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ. Ð‘ÐµÐ· ÐºÐ°Ð²Ñ‹Ñ‡ÐµÐº "Ð’Ð¾Ñ‚ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚"."""

    try:
        response = client.chat.completions.create(
            model=FLASH_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.9, # High creativity for variations
            max_tokens=800,
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"Paraphrase error: {e}")
        return text

def test_connection() -> Dict[str, Any]:
    """Test Gemini API connection"""
    try:
        client = get_client()
        response = client.chat.completions.create(
            model=FLASH_MODEL,
            messages=[{"role": "user", "content": "Say 'OK' if connection works"}],
            max_tokens=5
        )
        return {
            "status": "ok",
            "message": response.choices[0].message.content.strip(),
            "model": FLASH_MODEL
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

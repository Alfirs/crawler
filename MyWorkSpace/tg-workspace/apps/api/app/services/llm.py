"""
LLM Service for Message Generation and AI Coaching
Uses Google Gemini via OpenAI-compatible API
"""
import os
import json
import re
from typing import Dict, Any, List, Optional
from openai import OpenAI
import logging

logger = logging.getLogger(__name__)

GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"
FLASH_MODEL = "gemini-2.5-flash"
PRO_MODEL = "gemini-2.5-pro"


def get_client() -> OpenAI:
    """Get OpenAI client configured for Gemini"""
    api_key = os.getenv("GEMINI_API_KEY", "")
    if not api_key:
        raise ValueError("GEMINI_API_KEY not set")
    
    return OpenAI(api_key=api_key, base_url=GEMINI_BASE_URL)


def should_use_pro(context: Dict[str, Any]) -> bool:
    """
    Decide whether to use Pro model based on context complexity
    """
    # Use Pro for:
    # - Objection handling
    # - Complex conversations (multiple exchanges)
    # - High-value leads
    # - Coach recommendations
    
    if context.get('has_objection', False):
        return True
    if context.get('conversation_length', 0) > 3:
        return True
    if context.get('money_score', 0) > 0.8:
        return True
    if context.get('task_type') in ['coach', 'objection', 'strategy']:
        return True
    
    return False


def generate_outreach_message(
    lead_text: str,
    lead_author: str,
    category: str,
    template: Optional[str] = None,
    offers: Optional[List[str]] = None,
    previous_messages: Optional[List[str]] = None,
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate personalized outreach message for a lead
    Returns message text and metadata
    """
    client = get_client()
    context = context or {}
    model = PRO_MODEL if should_use_pro(context) else FLASH_MODEL
    
    # Build context for prompt
    offers_text = "\n".join(offers) if offers else "ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°: Ð±Ð¾Ñ‚Ñ‹, Ð¿Ð°Ñ€ÑÐµÑ€Ñ‹, Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸, CRM"
    prev_text = "\n".join(previous_messages) if previous_messages else "ÐŸÐµÑ€Ð²Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ"
    
    prompt = f"""Ð¢Ñ‹ - Ð¾Ð¿Ñ‹Ñ‚Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð´Ð°Ð²ÐµÑ† IT-ÑƒÑÐ»ÑƒÐ³. ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Ð¸Ð· Telegram.

ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢ Ð›Ð˜Ð”Ð:
ÐÐ²Ñ‚Ð¾Ñ€: {lead_author}
Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ: {lead_text}
ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ: {category}

ÐœÐžÐ˜ Ð£Ð¡Ð›Ð£Ð“Ð˜:
{offers_text}

ÐŸÐ Ð•Ð”Ð«Ð”Ð£Ð©ÐÐ¯ ÐŸÐ•Ð Ð•ÐŸÐ˜Ð¡ÐšÐ:
{prev_text}

{'Ð¨ÐÐ‘Ð›ÐžÐ Ð”Ð›Ð¯ ÐÐ”ÐÐŸÐ¢ÐÐ¦Ð˜Ð˜: ' + template if template else ''}

ÐŸÐ ÐÐ’Ð˜Ð›Ð:
1. Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¼ (2-4 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ)
2. ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð¿Ð¾Ð´ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ
3. ÐÐµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹ Ñ‚Ð¸Ð¿Ð° "Ð”Ð¾Ð±Ñ€Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ! Ð£Ð²Ð¸Ð´ÐµÐ» Ð²Ð°ÑˆÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ..."
4. ÐŸÐ¾ÐºÐ°Ð¶Ð¸ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¸Ð·Ñƒ, Ð½Ð¾ Ð½Ðµ Ñ…Ð²Ð°ÑÑ‚Ð°Ð¹ÑÑ
5. Ð—Ð°Ð´Ð°Ð¹ ÑƒÑ‚Ð¾Ñ‡Ð½ÑÑŽÑ‰Ð¸Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸Ð»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³
6. Ð¢Ð¾Ð½: Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¹, Ð½Ð¾ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹
7. ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÐ¼Ð¾Ð´Ð·Ð¸ Ð² Ð¸Ð·Ð±Ñ‹Ñ‚ÐºÐµ (Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ 1-2)
8. Ð”Ð¾Ð±Ð°Ð²ÑŒ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ (Ñ†Ð¸Ñ„Ñ€Ñ‹, ÐºÐµÐ¹ÑÑ‹, ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð¸ÐºÑƒ)

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð² JSON:
{{
    "message": "Ð¢ÐµÐºÑÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ",
    "hook": "Ð§Ñ‚Ð¾ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð·Ð°Ñ†ÐµÐ¿Ð¸Ð»Ð¸ Ð² Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ",
    "next_step": "ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³",
    "personalization_points": ["Ð¢Ð¾Ñ‡ÐºÐ° 1", "Ð¢Ð¾Ñ‡ÐºÐ° 2"]
}}"""

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=800,
        )
        
        content = response.choices[0].message.content.strip()
        
        # Parse JSON
        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            result = json.loads(json_match.group())
            result['model_used'] = model
            return result
        else:
            return {
                "message": content,
                "hook": "",
                "next_step": "",
                "personalization_points": [],
                "model_used": model
            }
            
    except Exception as e:
        logger.error(f"Message generation error: {e}")
        return {
            "message": "",
            "error": str(e),
            "model_used": model
        }


def handle_objection(
    objection_text: str,
    lead_context: str,
    previous_attempts: List[str] = None
) -> Dict[str, Any]:
    """
    Generate response to client objection
    Always uses Pro model for better quality
    """
    client = get_client()
    
    prev_text = "\n".join(previous_attempts) if previous_attempts else "ÐÐµÑ‚ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº"
    
    prompt = f"""ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð²Ñ‹Ð´Ð²Ð¸Ð½ÑƒÐ» Ð²Ð¾Ð·Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ. ÐŸÐ¾Ð¼Ð¾Ð³Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ.

ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢:
{lead_context}

Ð’ÐžÐ—Ð ÐÐ–Ð•ÐÐ˜Ð• ÐšÐ›Ð˜Ð•ÐÐ¢Ð:
{objection_text}

ÐŸÐ Ð•Ð”Ð«Ð”Ð£Ð©Ð˜Ð• ÐžÐ¢Ð’Ð•Ð¢Ð«:
{prev_text}

Ð”Ð°Ð¹ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸ÑŽ Ð¿Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ Ð²Ð¾Ð·Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°.
ÐžÑ‚Ð²ÐµÑ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½:
1. ÐŸÑ€Ð¸Ð·Ð½Ð°Ñ‚ÑŒ Ñ‚Ð¾Ñ‡ÐºÑƒ Ð·Ñ€ÐµÐ½Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
2. ÐœÑÐ³ÐºÐ¾ Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð²Ð¾Ð·Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ
3. Ð”Ð°Ñ‚ÑŒ Ñ†ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð¸Ð»Ð¸ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚
4. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³

JSON Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚:
{{
    "objection_type": "Ñ†ÐµÐ½Ð°/ÑÑ€Ð¾ÐºÐ¸/Ð´Ð¾Ð²ÐµÑ€Ð¸Ðµ/Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚/Ð´Ñ€ÑƒÐ³Ð¾Ðµ",
    "response": "Ð¢ÐµÐºÑÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°",
    "strategy": "ÐšÐ°ÐºÑƒÑŽ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»Ð¸",
    "alternative_responses": ["Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2", "Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 3"],
    "tips": ["Ð¡Ð¾Ð²ÐµÑ‚ 1", "Ð¡Ð¾Ð²ÐµÑ‚ 2"]
}}"""

    try:
        response = client.chat.completions.create(
            model=PRO_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=1000,
        )
        
        content = response.choices[0].message.content.strip()
        json_match = re.search(r'\{[\s\S]*\}', content)
        
        if json_match:
            return json.loads(json_match.group())
        else:
            return {"response": content, "objection_type": "unknown"}
            
    except Exception as e:
        logger.error(f"Objection handling error: {e}")
        return {"error": str(e)}


def get_sales_coach_advice(
    lead_info: Dict[str, Any],
    current_status: str,
    history: List[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Get AI Sales Coach recommendation for next action
    """
    client = get_client()
    
    history_text = json.dumps(history, ensure_ascii=False, indent=2) if history else "ÐÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸"
    
    prompt = f"""Ð¢Ñ‹ AI Sales Coach. ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð»Ð¸Ð´ Ð¸ Ð´Ð°Ð¹ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸.

Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð¯ Ðž Ð›Ð˜Ð”Ð•:
{json.dumps(lead_info, ensure_ascii=False, indent=2)}

Ð¢Ð•ÐšÐ£Ð©Ð˜Ð™ Ð¡Ð¢ÐÐ¢Ð£Ð¡: {current_status}

Ð˜Ð¡Ð¢ÐžÐ Ð˜Ð¯ Ð’Ð—ÐÐ˜ÐœÐžÐ”Ð•Ð™Ð¡Ð¢Ð’Ð˜Ð™:
{history_text}

Ð”Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ðµ, actionable Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸:
1. Ð§Ñ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ð´Ð°Ð»ÑŒÑˆÐµ?
2. ÐšÐ°ÐºÐ¾Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ?
3. ÐšÐ°ÐºÐ¸Ðµ Ñ€Ð¸ÑÐºÐ¸ ÑƒÑ‡ÐµÑÑ‚ÑŒ?
4. ÐžÑ†ÐµÐ½ÐºÐ° Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ ÑƒÑÐ¿ÐµÑ…Ð°

JSON:
{{
    "next_action": "ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ",
    "approach": "Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´",
    "timing": "ÐšÐ¾Ð³Ð´Ð° Ð»ÑƒÑ‡ÑˆÐµ ÑÐ²ÑÐ·Ð°Ñ‚ÑŒÑÑ",
    "risks": ["Ð Ð¸ÑÐº 1"],
    "success_probability": 0.7,
    "one_liner_tip": "ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ ÑÐ¾Ð²ÐµÑ‚ Ð² Ð¾Ð´Ð½Ð¾ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ"
}}"""

    try:
        response = client.chat.completions.create(
            model=FLASH_MODEL,  # Coach can use Flash for speed
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=600,
        )
        
        content = response.choices[0].message.content.strip()
        json_match = re.search(r'\{[\s\S]*\}', content)
        
        if json_match:
            return json.loads(json_match.group())
        else:
            return {"one_liner_tip": content}
            
    except Exception as e:
        logger.error(f"Coach advice error: {e}")
        return {"error": str(e)}


def generate_daily_summary(stats: Dict[str, Any]) -> str:
    """
    Generate end-of-day summary with AI insights
    """
    client = get_client()
    
    prompt = f"""Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ðµ (2-3 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ) ÐµÐ¶ÐµÐ´Ð½ÐµÐ²Ð½Ð¾Ðµ Ñ€ÐµÐ·ÑŽÐ¼Ðµ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ CRM.

Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ Ð”ÐÐ¯:
- ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹: {stats.get('messages_sent', 0)}
- ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²: {stats.get('replies', 0)}
- Ð”Ð²Ð¸Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ð²Ð¾Ñ€Ð¾Ð½ÐºÐµ: {stats.get('funnel_moves', 0)}
- ÐÐ¾Ð²Ñ‹Ñ… Ð»Ð¸Ð´Ð¾Ð²: {stats.get('new_leads', 0)}
- Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¾ ÑÐ´ÐµÐ»Ð¾Ðº: {stats.get('won', 0)}

Ð ÐµÐ·ÑŽÐ¼Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾:
1. ÐŸÐ¾Ñ…Ð²Ð°Ð»Ð¸Ñ‚ÑŒ Ð·Ð° ÑƒÑÐ¿ÐµÑ…Ð¸ Ð¸Ð»Ð¸ Ð¼ÑÐ³ÐºÐ¾ Ð¼Ð¾Ñ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ
2. Ð”Ð°Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ ÑÐ¾Ð²ÐµÑ‚ Ð½Ð° Ð·Ð°Ð²Ñ‚Ñ€Ð°
3. Ð‘Ñ‹Ñ‚ÑŒ Ð¿Ð¾Ð·Ð¸Ñ‚Ð¸Ð²Ð½Ñ‹Ð¼, Ð½Ð¾ Ð½Ðµ Ð¿Ñ€Ð¸Ñ‚Ð¾Ñ€Ð½Ñ‹Ð¼

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ Ñ€ÐµÐ·ÑŽÐ¼Ðµ, Ð±ÐµÐ· JSON."""

    try:
        response = client.chat.completions.create(
            model=FLASH_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            max_tokens=200,
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"Summary generation error: {e}")
        return f"Ð¡ÐµÐ³Ð¾Ð´Ð½Ñ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ {stats.get('messages_sent', 0)} ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹. ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°Ð¹ Ð² Ñ‚Ð¾Ð¼ Ð¶Ðµ Ð´ÑƒÑ…Ðµ! ðŸ’ª"


def uniqualize_message(template: str, variations: int = 3) -> List[str]:
    """
    Generate unique variations of a message template
    For anti-spam protection
    """
    client = get_client()
    
    prompt = f"""Ð¡Ð¾Ð·Ð´Ð°Ð¹ {variations} ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð° ÑÑ‚Ð¾Ð³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ.
Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸ ÑÐ¼Ñ‹ÑÐ», Ð½Ð¾ Ð¸Ð·Ð¼ÐµÐ½Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ, ÑÐ»Ð¾Ð²Ð°, Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº.
ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð²Ñ‹Ð³Ð»ÑÐ´ÐµÑ‚ÑŒ ÐºÐ°Ðº Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð½Ñ‹Ð¹ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ.

ÐžÐ Ð˜Ð“Ð˜ÐÐÐ›:
{template}

ÐžÑ‚Ð²ÐµÑ‚ÑŒ JSON ÑÐ¿Ð¸ÑÐºÐ¾Ð¼:
["Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 1", "Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2", "Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 3"]"""

    try:
        response = client.chat.completions.create(
            model=FLASH_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.9,
            max_tokens=1000,
        )
        
        content = response.choices[0].message.content.strip()
        json_match = re.search(r'\[[\s\S]*\]', content)
        
        if json_match:
            return json.loads(json_match.group())
        else:
            return [template]
            
        return [template]

def test_connection() -> Dict[str, Any]:
    """Test Gemini API connection"""
    try:
        client = get_client()
        response = client.chat.completions.create(
            model=FLASH_MODEL,
            messages=[{"role": "user", "content": "Say 'OK' if connection works"}],
            max_tokens=5
        )
        return {
            "status": "ok",
            "message": response.choices[0].message.content.strip(),
            "model": FLASH_MODEL
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

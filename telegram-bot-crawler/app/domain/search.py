from __future__ import annotations

from dataclasses import dataclass
from typing import List
import json
from pathlib import Path
from app.storage.db import Database
from app.config import AppConfig

@dataclass
class SearchResult:
    code: str
    description: str
    duty_pct: float
    confidence: float
    source: str # 'vector', 'fts', 'static'

class SearchService:
    def __init__(self, db: Database, config: AppConfig) -> None:
        self.db = db
        self.config = config
        
        # Initialize OpenAI client if key is present
        self.aclient = None
        if config.openrouter_api_key and config.openrouter_api_key != "STOPPED_EMERGENCY":
            try:
                from openai import AsyncOpenAI
                self.aclient = AsyncOpenAI(
                    api_key=config.openrouter_api_key,
                    base_url=config.openrouter_base_url,
                )
            except ImportError:
                print("OpenAI library not installed. LLM features disabled.")

    def ingest_data(self, json_path: str):
        """Loads TN VED data from JSON into the database."""
        path = Path(json_path)
        if not path.exists():
            print(f"File not found: {json_path}")
            return
            
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        self.db.execute_script("""
            CREATE TABLE IF NOT EXISTS tnved_codes (
                code TEXT PRIMARY KEY,
                description TEXT,
                duty_pct REAL,
                category TEXT
            );
        """)
        
        data_to_insert = []
        for item in data:
            data_to_insert.append((
                item["code"], 
                item["desc"], 
                item["duty"], 
                item.get("category", "other")
            ))
            
        self.db.executemany(
            "INSERT OR REPLACE INTO tnved_codes (code, description, duty_pct, category) VALUES (?, ?, ?, ?)",
            data_to_insert
        )
        print(f"Ingested {len(data_to_insert)} codes into DB.")

    async def search(self, query: str) -> List[SearchResult]:
        """
        Hybrid search:
        1. Exact match by Code (High confidence)
        2. SQL LIKE Search (Medium confidence)
        3. LLM (Disabled by default to save cost)
        """
        results = []
        placeholder = self.db.placeholder

        # 1. Direct Code Match
        clean_query = query.strip()
        row = self.db.fetchone(
            f"SELECT code, description, duty_pct FROM tnved_codes WHERE code = {placeholder}", 
            (clean_query,)
        )
        if row:
            results.append(SearchResult(
                code=row["code"],
                description=row["description"],
                duty_pct=row["duty_pct"],
                confidence=1.0,
                source="exact"
            ))
            return results

        # 2. Text Search (Smart Token Match)
        # Split query into tokens and filter
        import re
        tokens = [t.lower() for t in re.findall(r'\w+', clean_query) if len(t) > 2]
        
        # Get all candidates (table is small for MVP)
        rows = self.db.fetchall(f"SELECT code, description, duty_pct FROM tnved_codes")
        
        scored_results = []
        for row in rows:
            desc = row["description"].lower()
            # Simple scoring: how many tokens are present?
            # Basic stemming: "–¥–µ—Ç—Å–∫–∏–µ" -> "–¥–µ—Ç"
            match_count = 0
            for token in tokens:
                # Naive stem check
                root = token[:-2] if len(token) > 4 else token
                if root in desc:
                    match_count += 1
            
            if match_count > 0:
                # Calculate confidence based on coverage
                confidence = match_count / len(tokens) if tokens else 0
                if confidence > 0.4: # Tweak threshold
                    scored_results.append((confidence, row))
        
        # Sort by confidence
        scored_results.sort(key=lambda x: x[0], reverse=True)
        
        for score, row in scored_results[:5]:
            results.append(SearchResult(
                code=row["code"],
                description=row["description"],
                duty_pct=row["duty_pct"],
                confidence=score,
                source="fuzzy"
            ))

        
        # 3. LLM Search (If configured and local results are weak)
        # "Weak" = fewer than 3 results or top result < 0.8 confidence
        local_is_weak = len(results) < 3 or (results and results[0].confidence < 0.8)
        
        if self.aclient is not None and local_is_weak:
            try:
                print(f"[Search] Local results weak ({len(results)} items, top confidence: {results[0].confidence if results else 0}). Calling LLM...")
                llm_results = await self._ask_llm(query)
                
                if llm_results:
                    # If LLM returned good results, REPLACE weak local results
                    # (don't mix garbage local results with good LLM ones)
                    if llm_results[0].confidence >= 0.7:
                        print(f"[Search] LLM returned {len(llm_results)} results. Replacing weak local results.")
                        # Keep only high-confidence local results (exact matches)
                        good_local = [r for r in results if r.confidence >= 0.9]
                        results = good_local + llm_results
                    else:
                        # Merge normally if LLM also uncertain
                        existing_codes = {r.code for r in results}
                        for lr in llm_results:
                            if lr.code not in existing_codes:
                                results.append(lr)
            except Exception as e:
                print(f"LLM Search failed: {e}")

        # Final sort by confidence
        results.sort(key=lambda x: x.confidence, reverse=True)
        return results

    async def _ask_llm(self, query: str) -> List[SearchResult]:
        model = self.config.openrouter_model or "gpt-4o-mini"
        prompt = (
            f"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ç–∞–º–æ–∂–µ–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É –¢–ù –í–≠–î (HS Code) –ï–ê–≠–°.\n"
            f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏—â–µ—Ç —Ç–æ–≤–∞—Ä: ¬´{query}¬ª.\n\n"
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å 3 –ù–ê–ò–ë–û–õ–ï–ï –ü–û–î–•–û–î–Ø–©–ò–• 10-–∑–Ω–∞—á–Ω—ã—Ö –∫–æ–¥–∞ –¢–ù –í–≠–î –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –≤ –†–æ—Å—Å–∏—é.\n\n"
            "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:\n"
            "1. –ö–æ–¥—ã –î–û–õ–ñ–ù–´ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ç–æ–≤–∞—Ä—É –ø–æ —Å–º—ã—Å–ª—É!\n"
            "   - –û–¥–µ–∂–¥–∞ (–±—Ä—é–∫–∏, —é–±–∫–∏, –ø–ª–∞—Ç—å—è) ‚Üí –≥—Ä—É–ø–ø—ã 61-62\n"
            "   - –ë—Ä—é–∫–∏ –º—É–∂—Å–∫–∏–µ ‚Üí 6203 (–ù–ï —Å—É–º–∫–∏, –ù–ï —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞!)\n"
            "   - –Æ–±–∫–∏ –∂–µ–Ω—Å–∫–∏–µ ‚Üí 6204\n"
            "   - –û–±—É–≤—å ‚Üí –≥—Ä—É–ø–ø–∞ 64\n"
            "   - –ò–≥—Ä—É—à–∫–∏ ‚Üí 9503\n"
            "2. –ó–ê–ü–†–ï–©–ï–ù–û –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –∫–æ–¥—ã –∏–∑ –Ω–µ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π!\n"
            "3. –ü–æ—à–ª–∏–Ω–∞ (duty) –æ–±—ã—á–Ω–æ 0-15%\n\n"
            "–û—Ç–≤–µ—Ç ‚Äî —Å—Ç—Ä–æ–≥–æ JSON –º–∞—Å—Å–∏–≤:\n"
            '[{"code": "6203423500", "description": "–ë—Ä—é–∫–∏ –º—É–∂—Å–∫–∏–µ –∏–∑ —Ö–ª–æ–ø–∫–∞", "duty": 10.0, "confidence": 0.95}]\n\n'
            "–¢–û–õ–¨–ö–û JSON, –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞!"
        )
        
        try:
            print(f"[LLM] Calling {model} for query: '{query}'")
            resp = await self.aclient.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            content = resp.choices[0].message.content
            print(f"[LLM] Raw response: {content[:500]}...")
            
            # Clean markdown
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                 content = content.split("```")[1].split("```")[0]
            
            data = json.loads(content)
            results = []
            for item in data:
                code = str(item.get("code", "")).strip().replace(" ", "")
                desc = item.get("description", "AI Suggestion")
                duty_raw = float(item.get("duty", 10.0))
                # Normalize: if > 1, it's percentage (e.g., 10.0 = 10%)
                duty_pct = duty_raw / 100.0 if duty_raw > 1.0 else duty_raw
                confidence = float(item.get("confidence", 0.8))
                
                print(f"[LLM] Parsed: {code} - {desc[:50]}... (duty={duty_pct*100:.1f}%, conf={confidence})")
                
                results.append(SearchResult(
                    code=code,
                    description=desc,
                    duty_pct=duty_pct,
                    confidence=confidence,
                    source="ai"
                ))
            return results
        except Exception as e:
            print(f"[LLM] Error: {e}")
            return []

    async def check_certification(self, product_name: str, tnved_code: str | None) -> str:
        """
        Ask LLM about certification requirements (TR TS, Declaration, etc).
        """
        if not self.aclient:
             return "‚ö†Ô∏è API –∫–ª—é—á –¥–ª—è AI –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."

        model = self.config.openrouter_model or "gpt-4o-mini"
        code_str = f" (–ö–æ–¥ –¢–ù –í–≠–î: {tnved_code})" if tnved_code else ""
        query = f"{product_name}{code_str}"
        
        # 1. Try Local DB (Official Lists)
        if tnved_code and len(tnved_code) >= 4:
            # Find rules where prefix matches the start of our code
            # e.g. Rule '6204' matches Code '620432...'
            rows = self.db.fetchall(
                "SELECT doc_type, product_name, standard_doc FROM certification_rules WHERE ? LIKE tnved_prefix || '%' ORDER BY length(tnved_prefix) DESC LIMIT 5",
                (tnved_code,)
            )
            if rows:
                lines = ["‚úÖ **–ù–∞–π–¥–µ–Ω–æ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ—á–Ω—è—Ö –†–§:**"]
                seen = set()
                for r in rows:
                    key = (r['doc_type'], r['product_name'])
                    if key in seen: continue
                    seen.add(key)
                    
                    icon = "üìú" if "–î–µ–∫–ª–∞—Ä–∞—Ü–∏—è" in r['doc_type'] else "üõ°Ô∏è"
                    lines.append(f"{icon} **{r['doc_type']}**: {r['product_name']}")
                    if r['standard_doc']:
                         # Shorten doc string if too long
                         doc_short = r['standard_doc'][:100] + "..." if len(r['standard_doc']) > 100 else r['standard_doc']
                         lines.append(f"   üìÑ _{doc_short}_")
                
                lines.append("\nüëâ **–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ—á–Ω–æ:**")
                lines.append("[–ï–¥–∏–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä –ï–≠–ö](https://eec.eaeunion.org/comission/department/deptexreg/tr/TR_general.php)")
                return "\n".join(lines)

        # 2. AI Fallback
        prompt = (
            f"–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –í–≠–î. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –í–ï–†–û–Ø–¢–ù–´–ï —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n"
            f"–¢–æ–≤–∞—Ä: ¬´{query}¬ª.\n\n"
            "1. –ü–µ—Ä–µ—á–∏—Å–ª–∏ –¢–† –¢–° (–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã), –ø–æ–¥ –∫–æ—Ç–æ—Ä—ã–µ –ú–û–ñ–ï–¢ –ø–æ–¥–ø–∞–¥–∞—Ç—å —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä. (–ù–∞–ø—Ä–∏–º–µ—Ä, –¢–† –¢–° 004/2011, 020/2011, 017/2011 –∏ –¥—Ä.)\n"
            "2. –£–∫–∞–∂–∏ —Ñ–æ—Ä–º—É –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –∏–ª–∏ –î–µ–∫–ª–∞—Ä–∞—Ü–∏—è (–µ—Å–ª–∏ –æ—á–µ–≤–∏–¥–Ω–æ).\n"
            "3. –ï—Å–ª–∏ —Ç–æ–≤–∞—Ä —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ù–ï –ø–æ–¥–ª–µ–∂–∏—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–π —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–∏–ª–∏ –Ω—É–∂–Ω–æ –û—Ç–∫–∞–∑–Ω–æ–µ –ø–∏—Å—å–º–æ), —É–∫–∞–∂–∏ —ç—Ç–æ.\n\n"
            "–í–ê–ñ–ù–û: –¢—ã –¥–æ–ª–∂–µ–Ω —è–≤–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏—Ç—å, —á—Ç–æ —Å–ø–∏—Å–æ–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º.\n"
            "–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –Ω–∞—á–Ω–∏ –æ—Ç–≤–µ—Ç —Å —Ñ—Ä–∞–∑—ã: **¬´‚ö†Ô∏è –°–ø—Ä–∞–≤–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (AI). –í –±–∞–∑–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ—á–Ω–µ–π —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.¬ª**\n\n"
            "–ò—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–∂–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç, —Å–ø–∏—Å–∫–∏).\n"
            "–í –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–π:\n"
            "üëâ **–ì–¥–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ—á–Ω–æ:**\n"
            "[–ï–¥–∏–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä –ï–≠–ö](https://eec.eaeunion.org/comission/department/deptexreg/tr/TR_general.php) | [–†–µ–µ—Å—Ç—Ä –†–æ—Å–∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏–∏](https://pub.fsa.gov.ru/rss/certificate)"
        )

        
        try:
            print(f"[LLM-Cert] Calling {model} for: '{query}'")
            resp = await self.aclient.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1 # Lower temperature for less creativity
            )
            return resp.choices[0].message.content
        except Exception as e:
            print(f"[LLM-Cert] Error: {e}")
            return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ AI —Å–µ—Ä–≤–∏—Å—É —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

